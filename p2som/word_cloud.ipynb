{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MUQING WEN, ZHOUYI QIAN**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "#### Week 2: Visualizing word embeddings with Self-Organizing Maps\n",
    "\n",
    "# Project 2: Word Embeddings and Self-Organizing Maps (SOMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import som\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement a Self-Organizing Map (SOM) neural network\n",
    "\n",
    "You will implement a SOM to visualize the word embeddings in a 2D plot (\"word cloud\") and use the Iris dataset as a toy dataset to test your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Load and preprocess Iris\n",
    "\n",
    "Preprocess the data in the following way to ultimately produce `iris_x` (training data) and `iris_y` (classes):\n",
    "- Use all Iris features in the dataset except for \"species\". Normalize each row by its Euclidean distance so that the length of each data vector is 1.\n",
    "- Convert to \"species\" column (classes) to an int-code (e.g. values take on 0, 1, 2). *Might be helpful to consult your Project 0 from CS343.*\n",
    "- Make sure `iris_x` and `iris_y` are both ndarrays.\n",
    "\n",
    "Run the following test code to double-check your preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data cols except for species\n",
    "data = iris.loc[:, iris.columns != \"species\"].to_numpy()\n",
    "# normalize each row data vector\n",
    "iris_x = data / np.sqrt(np.sum(data**2,axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode species type str as ints\n",
    "species =iris['species'].astype('category').cat.codes\n",
    "iris_y = species.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Iris preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples are 150 and classes are 150.\n",
      "They should both be 150.\n",
      "\n",
      "1st 2 rows of preprocessed iris:\n",
      "[[0.804 0.552 0.221 0.032]\n",
      " [0.828 0.507 0.237 0.034]]\n",
      "They should be:\n",
      "[[0.804 0.552 0.221 0.032]\n",
      " [0.828 0.507 0.237 0.034]]\n",
      "\n",
      "Your first 4 classes of iris:\n",
      "[0 0 0 0] and they should be\n",
      "[0 0 0 0]\n",
      "Your last 4 classes of iris:\n",
      "[2 2 2 2] and they should be\n",
      "[2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of samples are {iris_x.shape[0]} and classes are {len(iris_y)}.\\nThey should both be 150.\\n')\n",
    "print(f'1st 2 rows of preprocessed iris:\\n{iris_x[:2]}')\n",
    "print('They should be:\\n[[0.804 0.552 0.221 0.032]\\n [0.828 0.507 0.237 0.034]]\\n')\n",
    "print(f'Your first 4 classes of iris:\\n{iris_y[:4]} and they should be\\n[0 0 0 0]')\n",
    "print(f'Your last 4 classes of iris:\\n{iris_y[-4:]} and they should be\\n[2 2 2 2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Implement the SOM constructor\n",
    "\n",
    "The main job here is to initialize the wts to normalized random values.\n",
    "\n",
    "Write test code below by creating a SOM (call it `iris_som`) in the cell below with the following characteristics:\n",
    "- 3x3 grid size\n",
    "- Set the number of features equal to the number of features in the iris training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your SOM initial wt shape: (3, 3, 4). It should be (3, 3, 4)\n",
      "Length of one of your wts: 1.00. It should be very close to 1.0\n",
      "Summed length of all of your wts: 9.0. It should be 9.0.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "iris_som = som.SOM(map_sz=(3,3), num_feats=4)\n",
    "\n",
    "# Keep this test code\n",
    "test_wts = iris_som.get_wts()\n",
    "print(f'Your SOM initial wt shape: {test_wts.shape}. It should be (3, 3, 4)')\n",
    "print(f'Length of one of your wts: {np.sqrt(np.square(test_wts[1, 1]).sum()):.2f}. It should be very close to 1.0')\n",
    "print(f'Summed length of all of your wts: {np.sqrt(np.square(test_wts).sum(axis=2)).sum()}. It should be 9.0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Get the SOM best matching unit for the 1st Iris sample\n",
    "\n",
    "Implement `get_bmu` to get the **best matching unit (BMU)** to a data sample. Recall: the BMU is the SOM neuron with the closest wts to the current data sample, measured by Euclidean distance.\n",
    "\n",
    "**Notes**:\n",
    "- In core SOM functions like this, you should not use loops of any kind. Your training code will be frustratingly slow if you use loops. Check out the method docstring to see whether loops are discouraged. *This is something that you could quantify as an extension.*\n",
    "- If vectorization is taking too much time to figure out, start with for loops and revisit this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_bmu`\n",
    "\n",
    "In the cell below, add two calls to the function to get the BMU for the FIRST and LAST Iris samples. This isn't meaningful yet because we have not trained the SOM yet.\n",
    "\n",
    "*Keep the code that replaces the wts in your SOM â€” this is for testing purposes, just to ensure that everyone's test code matches despite how you generate your SOM wts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.804 0.552 0.221 0.032]\n",
      "[0.69  0.351 0.597 0.211]\n"
     ]
    }
   ],
   "source": [
    "print(iris_x[0])\n",
    "print(iris_x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "np1 = np.array([[1, 2, 3], [4, 5, 6], [7, 9, 8]])\n",
    "print(np.unravel_index(np.argmax(np1), (3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMU for Iris sample 1 is at (1, 0) and should be at (1, 0).\n",
      "BMU for Iris sample 150 is at (0, 2) and should be at (0, 2).\n"
     ]
    }
   ],
   "source": [
    "# Keep this test code\n",
    "np.random.seed(0)\n",
    "iris_som.wts = np.random.random_sample(iris_som.wts.shape)\n",
    "bmu1 = iris_som.get_bmu(iris_x[0])\n",
    "bmu2 = iris_som.get_bmu(iris_x[-1])\n",
    "\n",
    "# Add your code to get the BMU for the 1st iris sample here\n",
    "\n",
    "print(f'BMU for Iris sample 1 is at {bmu1} and should be at (1, 0).')\n",
    "\n",
    "# Add your code to get the BMU for the last iris sample here\n",
    "\n",
    "print(f'BMU for Iris sample 150 is at {bmu2} and should be at (0, 2).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Find the nearest SOM wt vector to each current data sample\n",
    "\n",
    "Implement and test `get_nearest_wts` to \"automate\" the process of finding the closest wts to EVERY data sample in the dataset. In this case, return the set of closest wts rather than the indices of the SOM neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "np2 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(np2[(0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest wt vector to the 1st Iris sample [0.804 0.552 0.221 0.032] is\n",
      "[0.568 0.926 0.071 0.087] and should be\n",
      "[0.568 0.926 0.071 0.087]\n",
      "The nearest wt vector to the last Iris sample [0.69  0.351 0.597 0.211] is\n",
      "[0.964 0.383 0.792 0.529] and should be\n",
      "[0.964 0.383 0.792 0.529]\n"
     ]
    }
   ],
   "source": [
    "print(f'The nearest wt vector to the 1st Iris sample {iris_x[0]} is\\n{iris_som.get_nearest_wts(iris_x)[0]} and should be\\n[0.568 0.926 0.071 0.087]')\n",
    "print(f'The nearest wt vector to the last Iris sample {iris_x[-1]} is\\n{iris_som.get_nearest_wts(iris_x)[-1]} and should be\\n[0.964 0.383 0.792 0.529]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Define the Gaussian neighborhood\n",
    "\n",
    "Once we know the BMU for a particular data sample, we want to not only update the BMU's wts, but also the wts of its neighbors in the SOM grid (albeit to a weaker extent). Before we can update the BMU's wts, we need to be able to determine the Gaussian neighborhood surrounding the BMU. This is achieved by the function `gaussian`.\n",
    "\n",
    "Implement and test `gaussian` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [2 2 2 2]]\n",
      "[[0 1 2 3]\n",
      " [0 1 2 3]\n",
      " [0 1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "row_grid, col_grid = np.meshgrid(np.arange(3), np.arange(4), indexing=\"ij\")\n",
    "print(row_grid)\n",
    "print(col_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.368 0.607 0.368]\n",
      " [0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]]\n",
      "The Test 1 Gaussian neighborhood should look like:\n",
      "[[0.368 0.607 0.368]\n",
      " [0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]]\n"
     ]
    }
   ],
   "source": [
    "cent_rc = (1, 1)\n",
    "sigma = 1.0\n",
    "print(iris_som.gaussian(cent_rc, sigma))\n",
    "print('The Test 1 Gaussian neighborhood should look like:')\n",
    "print('''[[0.368 0.607 0.368]\n",
    " [0.607 1.    0.607]\n",
    " [0.368 0.607 0.368]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]\n",
      " [0.082 0.135 0.082]]\n",
      "The Test 2 Gaussian neighborhood should look like:\n",
      "[[0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]\n",
      " [0.082 0.135 0.082]]\n"
     ]
    }
   ],
   "source": [
    "cent_rc = (0, 1)\n",
    "print(iris_som.gaussian(cent_rc, sigma))\n",
    "print('The Test 2 Gaussian neighborhood should look like:')\n",
    "print('''[[0.607 1.    0.607]\n",
    " [0.368 0.607 0.368]\n",
    " [0.082 0.135 0.082]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.135 1.    0.135]\n",
      " [0.018 0.135 0.018]\n",
      " [0.    0.    0.   ]]\n",
      "The Test 3 Gaussian neighborhood should look like:\n",
      "[[0.135 1.    0.135]\n",
      " [0.018 0.135 0.018]\n",
      " [0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "cent_rc = (0, 1)\n",
    "sigma = 0.5\n",
    "print(iris_som.gaussian(cent_rc, sigma))\n",
    "print('The Test 3 Gaussian neighborhood should look like:')\n",
    "print('''[[0.135 1.    0.135]\n",
    " [0.018 0.135 0.018]\n",
    " [0.    0.    0.   ]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Update BMU wts\n",
    "\n",
    "Implement `update_wts` to bring the wts of the BMU and its neighbors closer to the current data sample. The SOM update rule is\n",
    "\n",
    "$$\\vec{w_{rc}}(t) = \\vec{w_{rc}}(t-1) + \\alpha g_{r_{bmu}, c_{bmu}}(r, c)\\left ( \\vec{x_i} - \\vec{w_{rc}}(t-1)\\right )$$\n",
    "\n",
    "Above, $w_{rc}$ is the SOM weight vector belonging to the unit positioned at row $r$ and column $c$, $t$ is iteration number, $g_{r_{bmu}, c_{bmu}}(r, c)$ is the Gaussian neighborhood matrix centered on the BMU evaluated at the SOM $(row, col) = (r, c)$, $x_i$ is the data vector, and $\\alpha$ is the learning rate.\n",
    "\n",
    "**Note:** This weight update occurs to ALL SOM unit rows and columns $(r, c)$ because of the Gaussian neighborhood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `update_wts`\n",
    "\n",
    "In the cell below, call the wt update method on the FIRST Iris sample, plugging in the row/column of the BMU determined above (*before the Gaussian tests*), and a learning rate of `0.1` and Gaussian sigma of `1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your SOM wts after updating the wts based on the 1st Iris sample are:\n",
      "[[[0.564 0.705 0.58  0.514]\n",
      "  [0.438 0.642 0.43  0.86 ]\n",
      "  [0.962 0.385 0.787 0.525]]\n",
      "\n",
      " [[0.592 0.888 0.086 0.082]\n",
      "  [0.068 0.816 0.744 0.819]\n",
      "  [0.976 0.796 0.458 0.77 ]]\n",
      "\n",
      " [[0.16  0.635 0.148 0.889]\n",
      "  [0.532 0.42  0.263 0.747]\n",
      "  [0.459 0.568 0.02  0.613]]]\n",
      "\n",
      "The output should be:\n",
      "[[[0.564 0.705 0.58  0.514]\n",
      "  [0.438 0.642 0.43  0.86 ]\n",
      "  [0.962 0.385 0.787 0.525]]\n",
      "\n",
      " [[0.592 0.888 0.086 0.082]\n",
      "  [0.068 0.816 0.744 0.819]\n",
      "  [0.976 0.796 0.458 0.77 ]]\n",
      "\n",
      " [[0.16  0.635 0.148 0.889]\n",
      "  [0.532 0.42  0.263 0.747]\n",
      "  [0.459 0.568 0.02  0.613]]]\n"
     ]
    }
   ],
   "source": [
    "# Keep the following test code\n",
    "np.random.seed(0)\n",
    "iris_som.wts = np.random.random_sample(iris_som.wts.shape)\n",
    "\n",
    "# Write your code here\n",
    "bmu1 = iris_som.get_bmu(iris_x[0])\n",
    "iris_som.update_wts(input_vector=iris_x[0], bmu_rc=bmu1, lr=0.1, sigma=1.0)\n",
    "\n",
    "# Keep the following test code\n",
    "test_wts = iris_som.get_wts()\n",
    "print('Your SOM wts after updating the wts based on the 1st Iris sample are:')\n",
    "print(test_wts)\n",
    "print('\\nThe output should be:')\n",
    "print('''[[[0.564 0.705 0.58  0.514]\n",
    "  [0.438 0.642 0.43  0.86 ]\n",
    "  [0.962 0.385 0.787 0.525]]\n",
    "\n",
    " [[0.592 0.888 0.086 0.082]\n",
    "  [0.068 0.816 0.744 0.819]\n",
    "  [0.976 0.796 0.458 0.77 ]]\n",
    "\n",
    " [[0.16  0.635 0.148 0.889]\n",
    "  [0.532 0.42  0.263 0.747]\n",
    "  [0.459 0.568 0.02  0.613]]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3g. Hyperparameter decay\n",
    "\n",
    "The last thing that needs to happen before we can train on multiple samples is to decay the network's hyperparameters (learning rate and neighborhood sigma). Implement multiplicative weight decay using the formula: $$p(t) = \\beta p(t-1)$$ where $\\beta$ is the decay rate.\n",
    "\n",
    "In the cell below, write a loop that decays a parameter 10 times from its initial value of 1 with a decay rate of 0.9. You should get:\n",
    "\n",
    "`0.9000 0.8100 0.7290 0.6561 0.5905 0.5314 0.4783 0.4305 0.3874 0.3487 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9000 0.8100 0.7290 0.6561 0.5905 0.5314 0.4783 0.4305 0.3874 0.3487 "
     ]
    }
   ],
   "source": [
    "val = 1\n",
    "rate = 0.9\n",
    "# print(f'{val:.4f}', end=' ')\n",
    "\n",
    "for i in range(10):\n",
    "    val = iris_som.decay_param(hyperparam=val, rate=rate)\n",
    "    print(f'{val:.4f}', end=' ')\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3h. Train SOM on Iris\n",
    "\n",
    "Implement the `fit` method to train a SOM for some number of epochs. \n",
    "\n",
    "Write code below that trains the SOM that you've been working with on the Iris dataset. Use the following hyperparameters:\n",
    "- `10` epochs.\n",
    "- learning rate of `2` and sigma of `10`.\n",
    "- learning rate decay of `0.999` and sigma decay of `0.999`.\n",
    "\n",
    "Signs that your code is working:\n",
    "- it takes no longer than a second or two to run.\n",
    "- you get 10 print outs (1 per epoch) showing the learning rate and sigma.\n",
    "- both the learning rate and sigma decrease across epochs.\n",
    "- your learning rate after 10 epochs should be `0.4459`.\n",
    "- your sigma after 10 epochs should be `2.2296`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train network ....\n",
      "Epoch 0/10, Learning rate: 1.9980%, Sigma: 9.9900%.\n",
      "Epoch 1/10, Learning rate: 1.9960%, Sigma: 9.9800%.\n",
      "Epoch 2/10, Learning rate: 1.9940%, Sigma: 9.9700%.\n",
      "Epoch 3/10, Learning rate: 1.9920%, Sigma: 9.9601%.\n",
      "Epoch 4/10, Learning rate: 1.9900%, Sigma: 9.9501%.\n",
      "Epoch 5/10, Learning rate: 1.9880%, Sigma: 9.9401%.\n",
      "Epoch 6/10, Learning rate: 1.9860%, Sigma: 9.9302%.\n",
      "Epoch 7/10, Learning rate: 1.9841%, Sigma: 9.9203%.\n",
      "Epoch 8/10, Learning rate: 1.9821%, Sigma: 9.9104%.\n",
      "Epoch 9/10, Learning rate: 1.9801%, Sigma: 9.9004%.\n"
     ]
    }
   ],
   "source": [
    "# Keep the code below\n",
    "np.random.seed(0)\n",
    "iris_som.wts = np.random.random_sample(iris_som.wts.shape)\n",
    "iris_x_cpy = iris_x.copy()\n",
    "\n",
    "# Your code here\n",
    "iris_som.fit(x = iris_x, n_epochs=10, lr=2, lr_decay=0.999, sigma=10, sigma_decay=0.999, print_every=1, verbose=True)\n",
    "\n",
    "# Keep the below code for testing\n",
    "if not np.allclose(iris_x, iris_x_cpy):\n",
    "    print('Uh oh, you changed the original training data, which is not good.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3i. Assessing performance with quantization error\n",
    "\n",
    "How well did you do at learning the structure of Iris?\n",
    "\n",
    "One way to answer this question is to compute the **BMU quantization error** (BMU error for short): On average, how much error do we incur by approximating each data sample by the weights of the corresponding BMU in the network? \n",
    "\n",
    "#### Todo\n",
    "\n",
    "- Implement the `error` method to determine the BMU error.\n",
    "- Go back to `fit` and when you print out training progress, print out the BMU error evaluated on the entire training set. If everything goes well, in the above fit test run with Iris, your BMU error steadily decrease over the 10 epochs (*There may be the occasional increase, but it should decrease on average over the 10 epochs*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3j. Visualize the learned structure of Iris by SOM\n",
    "\n",
    "One main advantage of using SOM is the ability to visualize the structure of high dimensional data in 2D. A trained SOM can be visualized by computing its **U-matrix**: the average distance between each neuron's weights and its 8 neighbors.\n",
    "\n",
    "Implement `u_matrix` so that you can visualize your network's representation of the Iris data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `u_matrix`\n",
    "\n",
    "In the cell below, do the following to visualize the learned representation of Iris across the SOM grid of neurons.\n",
    "- Train a 70x70 SOM on Iris for 4 epochs. Use an initial learning rate of 1 and sigma of 45. Set both decay factors to 0.99. This should complete in at most several seconds.\n",
    "- Compute the U-matrix and plot it as a grayscale image/heatmap. Add a colorbar.\n",
    "- Superimpose plot markers corresponding to each Iris sample (i.e. there should be 150 of them). Do this by determining the BMU of each data sample in the trained SOM, then drawing a plot marker at the BMU position. Color each marker according to the sample's true class. \n",
    "\n",
    "If all goes well, your U-matrix superimposed with the most active units for each training sample should (qualitatively) look like the lower-right image on [Wikipedia](https://en.wikipedia.org/wiki/Self-organizing_map#/media/File:SOM_of_Fishers_Iris_flower_data_set.JPG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this seed\n",
    "np.random.seed(2)\n",
    "\n",
    "som_sz = (70, 70)\n",
    "n_features = 4\n",
    "n_epochs = 4\n",
    "\n",
    "# Your code here\n",
    "\n",
    "markers = ['o','s','D']\n",
    "colors = ['b','g','r']\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3k. Questions\n",
    "\n",
    "**Question 3**: Explain what the above visualization is showing us. Why does the plot look different than Iris scatter plots like you're used to seeing? \n",
    "\n",
    "**Question 4**: Copy the plot with default hyperparameters into a separate cell (or save it in your project folder). How do the following affect the structure of the U-matrix?\n",
    "- Number of epochs\n",
    "- learning rate\n",
    "- Gaussian neighborhood sigma\n",
    "- SOM grid size\n",
    "\n",
    "**Question 5**: Change the random seed. Does it matter that the clusters may jump around? **Explain why**.\n",
    "\n",
    "**Question 6**: What does the dark band mean? What does it tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3**: \n",
    "\n",
    "**Answer 4**:\n",
    "\n",
    "**Answer 5**: \n",
    "\n",
    "**Answer 6**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Visualize IMDb word embeddings as a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Load word embedding vectors\n",
    "\n",
    "Run the cell below to load the Skip-gram word embeddings and the associated word strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('results/imdb_word_vectors.npy')\n",
    "word_strings = np.load('results/imdb_word_strings.npy')\n",
    "print(f'Loaded {len(word_vectors)} word embedding vectors and {len(word_strings)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Train SOM on word embedding vectors\n",
    "\n",
    "Below, train a new SOM on the word vectors. Training should take around 1 minute or less with the following suggested hyperparameters to get you started:\n",
    "- 100x100 size\n",
    "- 50 epochs\n",
    "- Initial learning rate of 3 with a decay rate of 0.9999.\n",
    "- Initial Gaussian neighborhood standard deviation of 100 with a decay rate of 0.9999.\n",
    "\n",
    "After training, create a grayscale heatmap of the U-matrix with a colorbar like you did for Iris (don't superimpose the data samples). You want to check and make sure that the U-matrix has structure to it (not look like salt-and-pepper pixel noise or be completely black/white/gray).\n",
    "\n",
    "**Notes:**\n",
    "- You are encouraged to experiment with the hyperparameters, the above values are starting points. *A more detailed investigation could be an extension.*\n",
    "- Training can be much, much slower if you have used for loops in the core algorithm (methods called on every iteration of SGD). If you're unable to do this, reduce the SOM grid size and/or number of training epochs, but be aware that your results may not be as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Produce IMDb review word cloud\n",
    "\n",
    "Run the following code to generate the word cloud. This is similar to what you did for Iris, except rather than showing colored plot markers, we are displaying the word at the position of its BMU in the SOM grid.\n",
    "\n",
    "Feel free to adjust the number of words visualized, font size, and the random offset of each BMU to help reduce words being plotted on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "num_words_to_visualize = 600\n",
    "word_inds = np.random.choice(len(word_vectors), size=num_words_to_visualize, replace=False)\n",
    "\n",
    "jitter = 1\n",
    "jitter_xy = np.random.uniform(low=-jitter, high=jitter, size=(num_words_to_visualize, 2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40, 40))\n",
    "for i, ind in enumerate(word_inds):\n",
    "    bmu_pos = word_som.get_bmu(word_vectors[ind])\n",
    "    # jitter the placement of words so that they are less likely to plot ontop of each other\n",
    "    x = bmu_pos[0] + 0.5 + jitter_xy[i, 0]\n",
    "    y = bmu_pos[1] + 0.5 + jitter_xy[i, 1]\n",
    "    ax.scatter(x, y)\n",
    "    ax.annotate(str(word_strings[ind]), (x, y), fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** What are your favorite word clusters?\n",
    "\n",
    "**Question 8:**\n",
    "\n",
    "(a) Are there any word groupings that surprised you (but make sense post-hoc)?\n",
    "\n",
    "(b) Are there words that you think should go together but are not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:**\n",
    "\n",
    "**Answer 8:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SOM quantization error\n",
    "\n",
    "Do an analysis (make plots, explain findings) where you investigate how various Skim-gram and/or SOM parameters affect the quantization error. Parameters you might experiment with are:\n",
    "- Number of epochs (Skip-gram and/or SOM)\n",
    "- SOM size\n",
    "- Learning rate (and its decay rate)\n",
    "- Gaussian sigma (and its decay rate)\n",
    "- Number of reviews used to train Skip-gram\n",
    "- Context word window size.\n",
    "\n",
    "**NOTE:** Some parameters may need to adjusted together in some fashion. For example, decreasing the SOM size decreases the grid size. Therefore, the Gaussian neighborhood size $\\sigma$ probably needs to be rescaled, as does the learning rate (how much vectors move around in the space duing each update)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. More sophisticated text preprocessing\n",
    "\n",
    "To preprocess text for Skipgram, we defined words as strings with at least one letter. Research and examine whether more sophisticated approaches (e.g. removing stop words, destemming, lemmatization, etc) to preprocessing text yield better word embedding results (better quality word context predictions, better word similiarity as represented by the SOM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Skip-gram word context\n",
    "\n",
    "We only used the Skip-gram network to extract the word embedding vectors. Load in the IMDb test set and explore how a trained Skip-gram network can predict words surrounding each target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Continuous Bag of Words (CBOW)\n",
    "\n",
    "In addition to Skip-gram, Mikolov et al. also proposed the CBOW model. Duplicate your Skipgram code and make the necessary modifications to implement CBOW. CBOW has the same overall architecture to Skip-gram, but the target and context words exchange roles in CBOW. In CBOW:\n",
    "- Context words (multi-hot coded) serve as the data samples.\n",
    "- Target words (one-hot coded) serve as the classes.\n",
    "\n",
    "Thus, the CBOW network is trained to \"fill in the blank\" given the context words.\n",
    "\n",
    "- How do the word vectors compare to Skipgram?\n",
    "- Analyze the target word predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Quality of SOM word clusters\n",
    "\n",
    "Experiment with how SOM learning parameters and Skip-gram training time (and other parameters like # embedding dimensions) affect the quality of similar word cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Other text datasets\n",
    "\n",
    "Obtain, load, and preprocess other text datasets. Train Skip-gram, visualize the word vectors using SOM, and interpret what you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Supervised learning with Learning Vector Quantization (LVQ)\n",
    "\n",
    "The SOM algorithm is closely related to LVQ, which can be applied to supervised learning problems. Create a child class of SOM that implements the LVQ algorithm and analyze its behavior when applied to a supervised learning dataset. There are numerous variants of LVQ (e.g. LVQ2, LVQ2.1, LVQ3, etc.). **Note:** Initialize the weights by randomly selecting samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Dimensionality reduction via SOM vs PCA\n",
    "\n",
    "Do a comparison between the dimensionality reduction offered by SOM vs PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Dimensionality reduction and visualization of non-text datasets\n",
    "\n",
    "SOM can be used to visualize and analyze the structure of high dimensional, non-text datasets. Find such datasets and apply SOM on them then analyze/interpret the structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
