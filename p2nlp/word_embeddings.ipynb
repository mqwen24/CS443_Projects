{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Muqing Wen, Leo Qian**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "CS 443: Bio-inspired Machine Learning\n",
    "\n",
    "#### Week 1: Natural language processing and word embeddings\n",
    "\n",
    "# Project 2: Word Embeddings and Self-Organizing Maps (SOMs)\n",
    "\n",
    "You will train a Skip-gram (word2vec) neural network commonly used in the field of natural language processing (NLP) on text from IMDb user movie reviews. The goal of the network is to predict **context words** (the words surrounding each word in a sentence). After implementing and training the network, you will extract the weights to obtain $H$ dimensional **word embedding** vectors for English words that appeared in the movie reviews. You will continue to leverage TensorFlow to implement and train the Skip-gram neural network on the IMDd dataset.\n",
    "\n",
    "In the second part of the project (`word_cloud.ipynb`), you will implement a self-organizing map (SOM), a competitive bio-inspired neural network that performs dimensionality reduction on its inputs. This network will learn the nonlinear structure of the IMDb word embeddings (in $H$ dimensions) (*unsupervised learning*) and allow you to visualize the words in a 2D space. Remarkably, words with similar meanings should appear nearby each other, even though the network knows nothing about the definitions of the words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn-v0_8-colorblind' not found in the style library and input is not a valid URL or path; see `style.available` for list of available styles",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/matplotlib/style/core.py:127\u001B[0m, in \u001B[0;36muse\u001B[0;34m(style)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 127\u001B[0m     rc \u001B[38;5;241m=\u001B[39m \u001B[43mrc_params_from_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_default_template\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m     _apply_style(rc)\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/matplotlib/__init__.py:854\u001B[0m, in \u001B[0;36mrc_params_from_file\u001B[0;34m(fname, fail_on_error, use_default_template)\u001B[0m\n\u001B[1;32m    840\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;124;03mConstruct a `RcParams` from file *fname*.\u001B[39;00m\n\u001B[1;32m    842\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;124;03m    parameters specified in the file. (Useful for updating dicts.)\u001B[39;00m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 854\u001B[0m config_from_file \u001B[38;5;241m=\u001B[39m \u001B[43m_rc_params_in_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfail_on_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfail_on_error\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_default_template:\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/matplotlib/__init__.py:780\u001B[0m, in \u001B[0;36m_rc_params_in_file\u001B[0;34m(fname, transform, fail_on_error)\u001B[0m\n\u001B[1;32m    779\u001B[0m rc_temp \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_or_url(fname) \u001B[38;5;28;01mas\u001B[39;00m fd:\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/contextlib.py:135\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/matplotlib/__init__.py:757\u001B[0m, in \u001B[0;36m_open_file_or_url\u001B[0;34m(fname)\u001B[0m\n\u001B[1;32m    756\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 757\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m f\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'seaborn-v0_8-colorblind'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimdb\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mword2vec\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstyle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mseaborn-v0_8-colorblind\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mseaborn-v0_8-darkgrid\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mrcParams\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfont.size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m18\u001B[39m})\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/matplotlib/style/core.py:130\u001B[0m, in \u001B[0;36muse\u001B[0;34m(style)\u001B[0m\n\u001B[1;32m    128\u001B[0m     _apply_style(rc)\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m not found in the style library and input is not a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid URL or path; see `style.available` for list of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavailable styles\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(style)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: 'seaborn-v0_8-colorblind' not found in the style library and input is not a valid URL or path; see `style.available` for list of available styles"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import imdb\n",
    "import word2vec\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Preprocess IMDb review dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Load in IMDb review data\n",
    "\n",
    "In the cell below, load in the movie review text data from `imdb_train.csv`. The goal is to get a Python list of length 25,000 (there are 25,000 reviews in the training set), where element $i$ is a single string representing the $i^{th}$ review. You're welcome to do this however you like. I suggest using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/imdb_train.csv\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(data[\"review\"].shape)\n",
    "review_list = list(data[\"review\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Extract a subset of the reviews\n",
    "\n",
    "Select the first 10 reviews (*non-shuffled*) from the dataset for development. You will parameterize this later to make it anything you would like for forthcoming neural network simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "first_10_review = review_list[0:10]\n",
    "print(len(first_10_review))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Create a corpus\n",
    "\n",
    "In NLP, we usually define a **corpus**, which is the set of documents from which we're interested in learning. For the IMDb dataset and Skip-gram network, this represents a list of sentences across all the reviews (*from the subset that you selected*).\n",
    "\n",
    "The problem is that the reviews are chunked by review, not sentence.\n",
    "- In `imdb.py` implement the `make_corpus` function to build a Python list where elements are sentences, not reviews.\n",
    "- In the cell below, call `make_corpus` on the 10 reviews, assign the resulting list of sentences as a variable called `corpus` for the below test code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99 sentences in the corpus. There should be 99.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "corpus = imdb.make_corpus(first_10_review)\n",
    "print(f'There are {len(corpus)} sentences in the corpus. There should be 99.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Identify unique words and the vocabulary size\n",
    "\n",
    "Now that you have a corpus of sentences, let's figure out the **vocabulary size**, defined as the number of unique words in the corpus (across all the sentences).\n",
    "- In `imdb.py` implement the `find_unique_words` function that returns a list of unique words in the corpus.\n",
    "- Call this function below on your corpus. Define `vocab_sz` as the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1016 words in the vocabulary. There should be 1016.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "unique_words = imdb.find_unique_words(corpus)\n",
    "vocab_sz = len(unique_words)\n",
    "print(f'There are {vocab_sz} words in the vocabulary. There should be 1016.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. Make word ↔ unique word index lookup tables\n",
    "\n",
    "It will be useful when training the neural network to assign an int code to each unique word in the range `[0, vocab_sz-1]`. In this subtask, create dictionaries to allow you to look up a word by its index and vice versa.\n",
    "\n",
    "In `imdb.py` implement the following functions:\n",
    "- `make_word2ind_mapping`: This makes a Python dictionary `word2ind` that allows you to use a word string to look up its int code.\n",
    "- `make_ind2word_mapping`: This makes a Python dictionary `ind2word` that allows you to use a word int code to look up its word string.\n",
    "\n",
    "In the below test code, `unique_words` refers to your Python list of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you preserved the order of words, the int code for \"robot\" is 159 and should be 159.\n",
      "If you preserved the order of words, the int code for \"fans\" is 108 and should be 108.\n",
      "If you preserved the order of words, the string associated with the int code 100 is call and should be call.\n",
      "If you preserved the order of words, the string associated with the int code 200 is buddy and should be buddy.\n"
     ]
    }
   ],
   "source": [
    "# Make word -> index lookup table\n",
    "word2ind = imdb.make_word2ind_mapping(unique_words)\n",
    "ind2word = imdb.make_ind2word_mapping(unique_words)\n",
    "print(f'If you preserved the order of words, the int code for \"robot\" is {word2ind[\"robot\"]} and should be 159.')\n",
    "print(f'If you preserved the order of words, the int code for \"fans\" is {word2ind[\"fans\"]} and should be 108.')\n",
    "      \n",
    "print(f'If you preserved the order of words, the string associated with the int code 100 is {ind2word[100]} and should be call.')\n",
    "print(f'If you preserved the order of words, the string associated with the int code 200 is {ind2word[200]} and should be buddy.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f. Make target word and context word sets for training a Skip-gram network\n",
    "\n",
    "To train the Skip-gram network, we need to create specially-formatted training data and classes.\n",
    "\n",
    "#### Target words\n",
    "\n",
    "Each training sample will be a single word in each sentence in the corpus (**target word**). Because the input layer of the network has $M$ units, you will one-hot code each target word: it is coded as a vector of zeros with length `vocab_sz` with a 1 at the position represented by the word's int code. For example, if `tourists` has an int code of 2, then its one-hot vector looks like `[0,0,1,0,0,0,0,....]`.\n",
    "\n",
    "Because it would consume large, unnecessary amounts of memory to preprocess every target word as a one-hot vector, for now you will represent each target word sample as its *int code* (order in vocabulary). You will convert the samples to one-hot vectors during training when forming mini-batches.\n",
    "\n",
    "#### Context words\n",
    "\n",
    "Recall that the goal of Skip-gram is to learn to predict **context words**, words that surround the target word (within a window of $W$ words) in a sentence. For example, if $W=2$, the sentence is `I want to see the new Star Wars movie, how about you?`, and the target word is `the`, then the context words are `[to, see, new, Star]`. Context words play the role of `y` (i.e. there are multiple \"classes\" per data sample). Eventually, you will represent each target word's set of context words as a \"*multi-hot vector*\": a vector of length `vocab_sz` with zeros everywhere except there are 1s at all the context word indices. As with target words, you will store the set as int codes for now (for efficiency) and convert to multi-hot vectors when forming mini-batches during training.\n",
    "\n",
    "#### Todo\n",
    "\n",
    "In `imdb.py`, implement the `make_target_context_word_lists` function that returns:\n",
    "1. an ndarray of int coded target words\n",
    "2. the associated context words, as an ndarray of ndarrays, each containing the int codes for the set of context words associated with the $i^{th}$ target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2255. It should be 2255.\n",
      "The first 10 target words indices are:\n",
      " [0 1 2 3 4 5 6 7 8 0]\n",
      "and should be\n",
      " [0 1 2 3 4 5 6 7 8 0]\n",
      "The last 10 target words indices are:\n",
      " [1012  282 1013 1014  454  275   70   41    7 1015]\n",
      "and should be\n",
      " [1012  282 1013 1014  454  275   70   41    7 1015]\n",
      "Second context list is [0 2 3] and should be [0 2 3]\n"
     ]
    }
   ],
   "source": [
    "targets_int, contexts_int = imdb.make_target_context_word_lists(corpus, word2ind, vocab_sz)\n",
    "print(f'Training size: {len(targets_int)}. It should be 2255.')\n",
    "print(f'The first 10 target words indices are:\\n {targets_int[:10]}\\nand should be\\n [0 1 2 3 4 5 6 7 8 0]')\n",
    "print(f'The last 10 target words indices are:\\n {targets_int[-10:]}\\nand should be\\n [1012  282 1013 1014  454  275   70   41    7 1015]')\n",
    "print(f'Second context list is {contexts_int[1]} and should be [0 2 3]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1g. Preprocess the IMDb dataset\n",
    "\n",
    "Write the function `get_imdb` in `imdb.py` to perform all the steps you've completed above in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 2255. It should be 2255.\n",
      "The first 10 target words indices are:\n",
      " [0 1 2 3 4 5 6 7 8 0]\n",
      "and should be\n",
      " [0 1 2 3 4 5 6 7 8 0]\n",
      "The last 10 target words indices are:\n",
      " [1012  282 1013 1014  454  275   70   41    7 1015]\n",
      "and should be\n",
      " [1012  282 1013 1014  454  275   70   41    7 1015]\n",
      "Second context list is [0 2 3] and should be [0 2 3]\n",
      "There are 1016 words in the vocabulary. There should be 1016.\n"
     ]
    }
   ],
   "source": [
    "num_reviews = 10\n",
    "targets_int, contexts_int, unique_words, _ = imdb.get_imdb('data/imdb_train.csv', num_reviews=num_reviews)\n",
    "\n",
    "print(f'Training size: {len(targets_int)}. It should be 2255.')\n",
    "print(f'The first 10 target words indices are:\\n {targets_int[:10]}\\nand should be\\n [0 1 2 3 4 5 6 7 8 0]')\n",
    "print(f'The last 10 target words indices are:\\n {targets_int[-10:]}\\nand should be\\n [1012  282 1013 1014  454  275   70   41    7 1015]')\n",
    "print(f'Second context list is {contexts_int[1]} and should be [0 2 3]')\n",
    "\n",
    "vocab_sz = len(unique_words)\n",
    "print(f'There are {vocab_sz} words in the vocabulary. There should be 1016.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement and train Skip-gram network\n",
    "\n",
    "The network has a MLP-like architecture:\n",
    "\n",
    "`Input -> Dense (linear act) -> Dense (softmax act)`.\n",
    "\n",
    "The number of input features (`M`) is the number of words in the vocabulary. The number of neurons in the output layer (`C`) is also the number of words in the vocabulary. So for Skip-gram, the number of input and output neurons are equal (i.e. `M` = `C`).\n",
    "\n",
    "You will build Skip-gram using the TensorFlow low-level API in `word2vec.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Skip-gram constructor and one-hot/multi-hot coding\n",
    "\n",
    "In the `Skipgram` class:\n",
    "- Copy-paste your code from `NeuralDecoder` in Project 1 to fill in the `one_hot` method. You will use this to encode each target word (*training sample*) when training the network.\n",
    "\n",
    "In addition, implement:\n",
    "- constructor\n",
    "- `set_wts` and `set_b`\n",
    "- `multi_hot` method: you will use this to encode the context words associated with each target word (*classes*) when training the network. You can think of the multi-hot coded context words as `y`, but in the case of Skip-gram there are *multiple* classes per sample (i.e. multiple correct answers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Your one-hot coding of [0 3 2 4] is:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]]\n",
      "and should be:\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 0 1]]\n",
      "onehotvecs.dtype=tf.float32 and should be tf.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 15:15:47.783355: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-09 15:15:47.783692: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y = np.array([0, 3, 2, 4])\n",
    "skip = word2vec.Skipgram(1, 1, C)\n",
    "onehotvecs = skip.one_hot(y, C)\n",
    "print(f'Your one-hot coding of {y} is:')\n",
    "tf.print(onehotvecs)\n",
    "print('and should be:\\n[[1 0 0 0 0]\\n [0 0 0 1 0]\\n [0 0 1 0 0]\\n [0 0 0 0 1]]')\n",
    "print(f'{onehotvecs.dtype=} and should be tf.float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: multi-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your multi-hot coding of [array([1]) array([3, 1]) array([2, 0, 1])] is:\n",
      "[[0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [1 1 1 0]]\n",
      "and should be:\n",
      "[[0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [1 1 1 0]]\n",
      "multihotvecs.dtype=tf.float32 and should be tf.float32\n"
     ]
    }
   ],
   "source": [
    "C = 4\n",
    "y = np.array([np.array([1]), np.array([3, 1]), np.array([2, 0, 1])], dtype=object)\n",
    "skip = word2vec.Skipgram(1, 1, C)\n",
    "multihotvecs = skip.multi_hot(y, C)\n",
    "print(f'Your multi-hot coding of {y} is:')\n",
    "tf.print(multihotvecs)\n",
    "print('and should be:')\n",
    "print('''[[0 1 0 0]\n",
    " [0 1 0 1]\n",
    " [1 1 1 0]]''')\n",
    "print(f'{multihotvecs.dtype=} and should be tf.float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Skip-gram forward pass and loss\n",
    "\n",
    "Implement and test the `forward` and `loss` methods.\n",
    "\n",
    "#### Skip-gram loss\n",
    "\n",
    "Equation for skip-gram loss for the current mini-batch: $$L = \\frac{1}{B}  \\sum_{i=1}^B  \\left ( S_i \\times Log\\left ( \\sum_{j=1}^{C} exp \\left ( \\text{zNetIn}_{ij} \\right ) \\right ) - \\sum_{k=1}^{C} y_{h_{ik}} \\text{zNetIn}_{ik} \\right ) $$\n",
    "\n",
    "**Left term:** $S_i$ is the number of words in the current context window (*remember, this is NOT constant across samples*), $\\text{zNetIn}_j$ are the `net_in` values in the Output layer, and the sum over $j$ is a sum over all `net_in` values for a given sample (summing over units coding each word in the vocab)\n",
    "\n",
    "**Right term:** This is the sum of the Output layer `net_in` values multiplied by each sample's multi-hot class vector to \"filter out\" only the `net_in` values at neurons coding the correct classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with simple corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"I love neural networks and I love computer science at colby\".split()]\n",
    "test_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_words = imdb.find_unique_words(test_corpus)\n",
    "test_vocab_sz = len(test_unique_words)\n",
    "# Make word -> index lookup table\n",
    "test_word2ind = imdb.make_word2ind_mapping(test_unique_words)\n",
    "test_ind2word = imdb.make_ind2word_mapping(test_unique_words)\n",
    "print('Your word2ind dictionary should look like:')\n",
    "print('''{'I': 0,\n",
    " 'love': 1,\n",
    " 'neural': 2,\n",
    " 'networks': 3,\n",
    " 'and': 4,\n",
    " 'computer': 5,\n",
    " 'science': 6,\n",
    " 'at': 7,\n",
    " 'colby': 8}''')\n",
    "print('Yours looks like:')\n",
    "test_word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training samples and labels\n",
    "test_targets_int, test_contexts_int = imdb.make_target_context_word_lists(test_corpus, test_word2ind, test_vocab_sz, context_win_sz=4)\n",
    "print(f'Training size: {len(test_targets_int)}. It should be 11.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consistent wts/bias so that everyone\n",
    "# gets consistent test code results\n",
    "test_y_wts = tf.constant(np.array([[0.2919751, 0.2065665],\n",
    "       [0.5353907, 0.5612575],\n",
    "       [0.4166745, 0.8078279],\n",
    "       [0.4932251, 0.9981292],\n",
    "       [0.6967351, 0.1253736],\n",
    "       [0.7098167, 0.6624156],\n",
    "       [0.5722566, 0.3647535],\n",
    "       [0.4205183, 0.630057 ],\n",
    "       [0.913813 , 0.6616472]]), dtype=tf.float32)\n",
    "test_y_b = tf.constant(np.array([0.2919751, 0.2065665]), dtype=tf.float32)\n",
    "\n",
    "test_z_wts = tf.constant(np.array([[0.2919751, 0.2065665, 0.5353907, 0.5612575, 0.4166745, 0.8078279,\n",
    "        0.4932251, 0.9981292, 0.6967351],\n",
    "       [0.1253736, 0.7098167, 0.6624156, 0.5722566, 0.3647535, 0.4205183,\n",
    "        0.630057 , 0.913813 , 0.6616472]]), dtype=tf.float32)\n",
    "test_z_b = tf.constant(np.array([0.2919751, 0.2065665, 0.5353907, 0.5612575, 0.4166745, 0.8078279,\n",
    "       0.4932251, 0.9981292, 0.6967351]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = test_vocab_sz  # 9\n",
    "H = 2\n",
    "C = test_vocab_sz\n",
    "\n",
    "skip = word2vec.Skipgram(num_feats=M,\n",
    "                         num_hidden=H,\n",
    "                         num_classes=C)\n",
    "skip.set_wts(test_y_wts, test_z_wts)\n",
    "skip.set_b(test_y_b, test_z_b)\n",
    "\n",
    "print('Your multi-hot encoding of the context words for all samples:')\n",
    "test_context_multihot = skip.multi_hot(test_contexts_int, len(test_unique_words))\n",
    "print(test_context_multihot)\n",
    "print('and it should be:')\n",
    "print('''tf.Tensor(\n",
    "[[0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
    " [1. 1. 0. 1. 1. 0. 0. 0. 0.]\n",
    " [1. 1. 1. 0. 1. 1. 0. 0. 0.]\n",
    " [1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
    " [0. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
    " [1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
    " [1. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
    " [1. 1. 0. 0. 1. 1. 0. 1. 1.]\n",
    " [1. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
    " [0. 1. 0. 0. 0. 1. 1. 1. 0.]], shape=(11, 9), dtype=float32)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward\n",
    "test_net_in = skip.forward(skip.one_hot(test_targets_int, test_vocab_sz))\n",
    "print(f'Your output layer net_in shape is:\\n{test_net_in.shape} and should be (11, 9)')\n",
    "print(f'The first sample net_in is \\n{test_net_in[0]}\\nand should be')\n",
    "print('''[0.51427   0.6204398 1.1216979 1.125422  0.8106834 1.4532892 1.0415413\n",
    " 1.9585133 1.3769419]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss\n",
    "test_loss = skip.loss(test_net_in, test_context_multihot)\n",
    "print(f'Your loss is {test_loss:.6f} and it should be 13.284546')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Skip-gram fit\n",
    "\n",
    "Copy-and-paste your code from `extract_at_indices` in Project 1 into the method of the same name in the `Skipgram` class.\n",
    "\n",
    "Next, write the `fit` method. This method is simpler than what you worked on in Project 1 — there is no validation set and no early stopping (training runs for a fixed number of epochs). This is because our primary goal is to learn the training set well (i.e. overfit) so that we can get good word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test training workflow\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "- Create a new `Skipgram` network with 10 hidden units and wt standard deviation of 1e-1.\n",
    "- Fit the model to `test_targets_int`, `test_contexts_int` with 500 epochs of training, batch gradient descent (mini-batch size of 11), and learning rate of 1e-2.\n",
    "    \n",
    "If your network works, you should see the loss drop to ~9.77-9.78 after about 150 epochs and remain stable for the rest of the simulation (no large jumps). This should take at most seconds to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Questions\n",
    "\n",
    "**Question 1:** How do you control the dimension of the word vector embedding with the Skip-gram network?\n",
    "\n",
    "**Question 2:** What is the relationship between the number of input and output neurons in Skip-gram?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** \n",
    "\n",
    "**Answer 2:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Train your model on IMDb data\n",
    "\n",
    "Set up and train a Skip-gram network on the IMDb movie review data:\n",
    "- Load in the IMDb data, form a corpus for the training set based on the 1st 15 reviews.\n",
    "- Train a Skip-gram neural on the corpus. Use a 20-dimensional word embedding (*you can change this later*). 400 epochs should be enough with the default learning rate.\n",
    "- Create a plot of your training loss over epochs.\n",
    "    \n",
    "If all goes well, the training loss should plateau at 11-12. Training should take no longer than a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Get word embedding vectors\n",
    "\n",
    "In the `Skipgram` class, implement:\n",
    "\n",
    "- the `get_word_vector` method that allows us to get the embedding vector for a single word from the trained network.\n",
    "- the `get_all_word_vectors` method that allows us to get all word embedding vectors from a list passed in to the trained network.\n",
    "\n",
    "Run the following code to test and save off the IMDb word embeddings. *Adjust variable names as needed to match your naming scheme.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_word_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of one of your word vectors are {net.get_word_vector(word2ind, \"james\").shape} and should be (20,)')\n",
    "print(f'The word vector for \"james\" looks like:\\n{net.get_word_vector(word2ind, \"james\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_all_word_vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = net.get_all_word_vectors(word2ind, unique_words)\n",
    "print(f'Shape of word vectors: {word_vecs.shape} and should be (1284, 20)\\n(for 15 reviews and embedding sz of 20)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g. Save the word vectors and unique word strings for visualization\n",
    "\n",
    "In the next notebook (`word_cloud.ipynb`), you will implement a self-organizing map (SOM) to visualize your IMDb word vectors. The number of reviews that you include in the training set and the hyperparameters used when training the Skip-gram network (e.g.  embedding dimension) will affect the quality of the visualization. To start, you could use the 15 reviews and training configuration from above. **If you decide to make changes, copy and paste your IMDb preprocessing and network training code below and modify it there (so that the above test still produces the expected results).**\n",
    "\n",
    "Run the following code to export the word vectors and unique words to the `results` folder. Change variable names as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/imdb_word_vectors', word_vecs)\n",
    "np.save('results/imdb_word_strings', unique_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
